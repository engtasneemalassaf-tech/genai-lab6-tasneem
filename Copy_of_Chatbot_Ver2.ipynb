{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engtasneemalassaf-tech/genai-lab6-tasneem/blob/main/Copy_of_Chatbot_Ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osBAXfbYP2YC"
      },
      "source": [
        "# Generative AI Study Assistant using RAG\n",
        "\n",
        "**Team Name:** Rahaf Kanaan, Shifaa Al-zu'bi, Thabet Zamari, Rafah Ali, Tasneem Alassaf.\n",
        "\n",
        "This Jupyter Notebook presents a Retrieval-Augmented Generation (RAG) based study assistant developed as part of a Generative AI course project.  \n",
        "The system is designed to answer student questions by retrieving relevant information from course lecture materials and generating accurate context-aware responses using large language models.\n",
        "\n",
        "The notebook demonstrates the complete pipeline, including document preprocessing, semantic retrieval, prompt engineering, and response generation.  \n",
        "Both API-based and local open-source language models are supported, allowing flexibility in experimentation while maintaining the same RAG architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QruO7EkAplVp"
      },
      "source": [
        "## 1. Environment Setup and Library Installation\n",
        "\n",
        "In this step, we install all the required Python libraries needed to build the Generative AI project.  \n",
        "These libraries support:\n",
        "\n",
        "- LangChain framework for building the RAG pipeline\n",
        "- Document loading and text splitting\n",
        "- Embedding models and vector storage using FAISS\n",
        "- Integration with OpenAI / GitHub Models APIs\n",
        "- Evaluation utilities and PDF processing\n",
        "\n",
        "This setup ensures that the environment contains all dependencies before starting the implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4pYBqu5j_VE",
        "outputId": "d56c073e-4257-4e4b-88d4-ba6892769686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip -q install -U langchain langchain-community langchain-text-splitters\n",
        "\n",
        "!pip -q install -U faiss-cpu sentence-transformers\n",
        "\n",
        "!pip -q install -U langchain-openai tiktoken python-dotenv\n",
        "\n",
        "!pip -q install -U rouge-score\n",
        "\n",
        "!pip install pypdf\n",
        "\n",
        "!pip -q install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTMkc8iuKOkR"
      },
      "source": [
        "## 2. Language Model Selection and Initialization\n",
        "\n",
        "This step initializes the Large Language Model (LLM) used for answer generation.  \n",
        "The implementation is designed to be flexible, allowing the system to switch between:\n",
        "\n",
        "- **API-based models** (GPT-4o via GitHub Models API) for advanced reasoning and explanation.\n",
        "- **Local open-source models** (flan-t5-base) for offline inference without external API dependencies.\n",
        "\n",
        "The selection is controlled using a configuration variable (`LLM_MODE`), enabling seamless comparison between different LLM backends while keeping the rest of the RAG pipeline unchanged.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoANL3r1JFrU",
        "outputId": "33711456-9488-4d1d-d005-e242e641905d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM loaded: Flan-T5-base (local HuggingFace)\n"
          ]
        }
      ],
      "source": [
        "LLM_MODE = \"local\"\n",
        "\n",
        "\n",
        "if LLM_MODE == \"api\":\n",
        "    from getpass import getpass\n",
        "    import os\n",
        "    from langchain_openai import ChatOpenAI\n",
        "\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your GitHub PAT: \")\n",
        "\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        base_url=\"https://models.github.ai/inference/v1\",\n",
        "        api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    print(\"LLM loaded: GPT-4o via GitHub Models API\")\n",
        "\n",
        "elif LLM_MODE == \"local\":\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "    from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "    model_name = \"google/flan-t5-base\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    print(\"LLM loaded: Flan-T5-base (local HuggingFace)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE-XQxbMqEoe"
      },
      "source": [
        "## 3. Uploading Course Materials (PDF Files)\n",
        "\n",
        "In this step, the course lecture slides and reference materials are uploaded to the Google Colab environment.  \n",
        "These PDF files serve as the **knowledge source** for the Retrieval-Augmented Generation (RAG) system and will later be processed, indexed, and queried by the chatbot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "ih4C0wTqVc2g",
        "outputId": "dad5ddef-c9c8-498a-f43e-0623e7c295ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2855082a-2b62-4332-98ed-7cf2a16488a6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2855082a-2b62-4332-98ed-7cf2a16488a6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving HTU - CPD - GenAI - Module1.pdf to HTU - CPD - GenAI - Module1.pdf\n",
            "Saving HTU - CPD - GenAI - Module2-A.pdf to HTU - CPD - GenAI - Module2-A.pdf\n",
            "Saving HTU - CPD - GenAI - Module2-B.pdf to HTU - CPD - GenAI - Module2-B.pdf\n",
            "Saving HTU - CPD - GenAI - Module3.pdf to HTU - CPD - GenAI - Module3.pdf\n",
            "Saving HTU - CPD - GenAI - Module4.pdf to HTU - CPD - GenAI - Module4.pdf\n",
            "Saving HTU - CPD - GenAI - Module5.pdf to HTU - CPD - GenAI - Module5.pdf\n",
            "Saving HTU - CPD - GenAI - Module6.pdf to HTU - CPD - GenAI - Module6.pdf\n",
            "Saving HTU - CPD - GenAI - Module6-B-RAG.pdf to HTU - CPD - GenAI - Module6-B-RAG.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbnCeJH5qOLU"
      },
      "source": [
        "## 4. Organizing Uploaded Files into a Data Directory\n",
        "\n",
        "After uploading the PDF files, they are organized into a dedicated directory (`data/`).  \n",
        "This step ensures a clean and structured project layout, making it easier to load, process, and manage the documents consistently throughout the pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPOqxz_zXyNQ",
        "outputId": "db1bd9ca-e8e0-43d0-ee6f-4280810e4a29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files inside data: ['HTU - CPD - GenAI - Module2-A.pdf', 'HTU - CPD - GenAI - Module6.pdf', 'HTU - CPD - GenAI - Module3.pdf', 'HTU - CPD - GenAI - Module5.pdf', 'HTU - CPD - GenAI - Module1.pdf', 'HTU - CPD - GenAI - Module2-B.pdf', 'HTU - CPD - GenAI - Module6-B-RAG.pdf', 'HTU - CPD - GenAI - Module4.pdf']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "for file in os.listdir():\n",
        "    if file.endswith(\".pdf\"):\n",
        "        shutil.move(file, \"data/\" + file)\n",
        "\n",
        "print(\"Files inside data:\", os.listdir(\"data\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRWvUGXWqZiG"
      },
      "source": [
        "## 5. Loading and Parsing PDF Documents\n",
        "\n",
        "This step loads the uploaded PDF files from the data directory and extracts their textual content.  \n",
        "Each PDF is processed page by page, converting the raw documents into structured text objects that can be further analyzed and indexed by the RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfvnUySxYNky",
        "outputId": "0769b223-8318-4001-d46b-c3f5bd07f998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 470 pages from PDFs\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "DATA_PATH = \"data/\"\n",
        "documents = []\n",
        "\n",
        "for file_name in os.listdir(DATA_PATH):\n",
        "    if file_name.endswith(\".pdf\"):\n",
        "        file_path = os.path.join(DATA_PATH, file_name)\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        documents.extend(loader.load())\n",
        "\n",
        "print(f\"Loaded {len(documents)} pages from PDFs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOAMkb8Vqmqf"
      },
      "source": [
        "## 6. Text Chunking for Efficient Retrieval\n",
        "\n",
        "In this step, the extracted document text is divided into smaller overlapping chunks.  \n",
        "Chunking improves retrieval accuracy by allowing the system to match user queries with the most relevant portions of the documents rather than entire pages.\n",
        "\n",
        "The overlap between chunks helps preserve contextual continuity across adjacent text segments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPR96-39ZH3O",
        "outputId": "143b6256-f2dc-40b7-bdff-4d3b3054cc40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 581 text chunks\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Created {len(chunks)} text chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjnh7fgRqwlv"
      },
      "source": [
        "## 7. Creating Embeddings and Building the Vector Store\n",
        "\n",
        "In this step, semantic embeddings are generated for each text chunk using a pre-trained sentence transformer model.  \n",
        "These embeddings represent the meaning of the text in a numerical vector space, enabling effective similarity-based retrieval.\n",
        "\n",
        "The vectors are then stored in a FAISS index, which allows fast and efficient retrieval of the most relevant document chunks in response to user queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpXfG6ABalZ0"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "vectorstore = FAISS.from_documents(\n",
        "    chunks,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg08ILnTq7sq"
      },
      "source": [
        "## 8. Prompt Engineering and Instruction Design\n",
        "\n",
        "This step defines the prompt template that controls how the language model interprets and answers user questions.  \n",
        "The prompt is carefully designed to encourage clear reasoning and explanatory responses while strictly limiting the model to the provided course context.\n",
        "\n",
        "By specifying detailed instructions, the system ensures that answers remain accurate, grounded in the retrieved documents, and free from external or hallucinated information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQtgwoCKaK_c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a study assistant for a Generative AI course. \"\n",
        "        \"Your task is to answer questions strictly using the provided course context.\\n\\n\"\n",
        "        \"Instructions:\\n\"\n",
        "        \"- Carefully analyze the question, even if it is indirect, rephrased, or explanatory.\\n\"\n",
        "        \"- You are allowed to reason, explain, and infer logically, but ONLY using information from the context.\\n\"\n",
        "        \"- If the answer requires combining multiple parts of the context, do so clearly.\\n\"\n",
        "        \"- If the question cannot be fully answered using the context, say exactly:\\n\"\n",
        "        \"  'I don't know based on the course material.'\\n\"\n",
        "        \"- Do NOT use any external knowledge.\\n\"\n",
        "        \"- Do NOT leave the question unanswered.\\n\"\n",
        "        \"-if the question is hello, you can answer it: hello, how i can help you in the material.\\n\"\n",
        "        \"-if the question is thank you, you can answer You are welcome, have a nice day ^-^.\\n\"\n",
        "        \"- if the question is goodbye, you can answer it: goodbye, have a nice day.\\n\"\n",
        "        \"-if the question is exit, you can answer it: goodbye, have a nice day.\"\n",
        "    ),\n",
        "\n",
        "    (\n",
        "\n",
        "        \"human\",\n",
        "        \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\n\"\n",
        "        \"Please provide a clear, step-by-step explanation.\"\n",
        "    )\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xUHIEDMrL_3"
      },
      "source": [
        "## 9. Constructing the Retrieval-Augmented Generation (RAG) Pipeline\n",
        "\n",
        "This step combines all previously defined components into a single Retrieval-Augmented Generation (RAG) pipeline.  \n",
        "The pipeline retrieves the most relevant document chunks based on the user query, injects them into the prompt as context, and then generates a grounded response using the selected language model.\n",
        "\n",
        "This modular chain ensures a clear separation between retrieval, prompting, and generation stages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc_neG3jmyBS"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXelp47xVGSn"
      },
      "source": [
        "## 10. Graphical User Interface (GUI) for the Study Assistant\n",
        "\n",
        "This step extends the RAG-based study assistant with a simple and user-friendly graphical interface using Gradio.  \n",
        "The interface allows users to interact with the chatbot by entering questions through a text box and receiving responses in real time.\n",
        "\n",
        "This GUI demonstrates the practical usability of the system while relying on the same underlying RAG pipeline without any modification to the retrieval or generation components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "wI9jSHt-VRlm",
        "outputId": "b0930284-e327-4d80-db20-5df54927f30e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-595824410.py:10: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n",
            "/tmp/ipython-input-595824410.py:10: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n",
            "/tmp/ipython-input-595824410.py:88: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-595824410.py:88: DeprecationWarning: The 'show_copy_button' parameter will be removed in Gradio 6.0. You will need to use 'buttons=[\"copy\"]' instead.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-595824410.py:88: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/components/base.py:207: UserWarning: 'scale' value should be an integer. Using 0.1 will cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b9965ece168f514df8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://b9965ece168f514df8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def respond(message, history):\n",
        "    if message.strip() == \"\":\n",
        "        return history, \"\"\n",
        "    answer = rag_chain.invoke(message)\n",
        "    history.append((message, answer))\n",
        "    return history, \"\"\n",
        "\n",
        "with gr.Blocks(\n",
        "    title=\"Generative AI Chatbot Assistant\",\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=\"\"\"\n",
        "        body {\n",
        "            background: linear-gradient(135deg, #eef2ff, #f4f7fb);\n",
        "        }\n",
        "\n",
        "        .platform-header {\n",
        "            background: linear-gradient(90deg, #7f7cff, #a0a0ff); /* lighter gradient for logo */\n",
        "            padding: 30px;\n",
        "            border-radius: 14px;\n",
        "            color: white;\n",
        "            text-align: center;\n",
        "            margin-bottom: 25px;\n",
        "            box-shadow: 0 8px 20px rgba(0,0,0,0.12);\n",
        "        }\n",
        "\n",
        "        .platform-header h1 {\n",
        "            margin-bottom: 6px;\n",
        "            font-size: 32px;\n",
        "        }\n",
        "\n",
        "        .platform-header p {\n",
        "            font-size: 15px;\n",
        "            opacity: 0.95;\n",
        "            max-width: 700px;\n",
        "            margin: 0 auto;\n",
        "        }\n",
        "\n",
        "        .chat-card {\n",
        "            background: white;\n",
        "            border-radius: 14px;\n",
        "            padding: 20px;\n",
        "            box-shadow: 0 10px 30px rgba(0,0,0,0.08);\n",
        "        }\n",
        "\n",
        "        .footer-text {\n",
        "            text-align: center;\n",
        "            font-size: 12px;\n",
        "            color: #6b7280;\n",
        "            margin-top: 15px;\n",
        "        }\n",
        "\n",
        "        /* Send Button aligned with textbox */\n",
        "        button.primary {\n",
        "            background: linear-gradient(90deg, #7f7cff, #a0a0ff) !important;\n",
        "            border: none !important;\n",
        "            padding: 6px 16px !important;\n",
        "            font-size: 13px !important;\n",
        "            min-height: 36px !important;\n",
        "            min-width: 100px !important;\n",
        "        }\n",
        "    \"\"\"\n",
        ") as demo:\n",
        "\n",
        "    # Header with Logo\n",
        "    with gr.Column(elem_classes=\"platform-header\"):\n",
        "        gr.Image(\n",
        "            value=\"LogoGUI.png\",\n",
        "            height=100,\n",
        "            show_label=False,\n",
        "            show_download_button=False,\n",
        "            container=False\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <h1>Generative AI Study Assistant</h1>\n",
        "            <p>\n",
        "                A professional retrieval-augmented learning platform that delivers\n",
        "                accurate, context-aware answers from approved academic resources.\n",
        "            </p>\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    # Chat card\n",
        "    with gr.Column(elem_classes=\"chat-card\"):\n",
        "        chatbot = gr.Chatbot(\n",
        "            label=\"AI Assistant\",\n",
        "            height=300,\n",
        "            show_copy_button=True\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            user_input = gr.Textbox(\n",
        "                placeholder=\"Ask a question about the course content...\",\n",
        "                label=\"Your Question\",\n",
        "                scale=4\n",
        "            )\n",
        "            send_btn = gr.Button(\n",
        "                \"Send\",\n",
        "                variant=\"primary\",\n",
        "                scale=0.1\n",
        "            )\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <div class=\"footer-text\">\n",
        "            Powered by Retrieval-Augmented Generation (RAG).\n",
        "            Responses are generated exclusively from approved academic resources.\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Event handlers\n",
        "    send_btn.click(\n",
        "        fn=respond,\n",
        "        inputs=[user_input, chatbot],\n",
        "        outputs=[chatbot, user_input]\n",
        "    )\n",
        "\n",
        "    user_input.submit(\n",
        "        fn=respond,\n",
        "        inputs=[user_input, chatbot],\n",
        "        outputs=[chatbot, user_input]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Q7Tv1Arajw"
      },
      "source": [
        "## 10. Interactive Chatbot Interface\n",
        "\n",
        "In this final step, an interactive command-line chatbot is implemented.  \n",
        "Users can dynamically input questions related to the course material, and the system responds in real time using the constructed RAG pipeline.\n",
        "\n",
        "This interface demonstrates the practical application of the system as a study assistant rather than a static question-answering script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPkQ-gMBnbQe",
        "outputId": "cb97cb52-335c-4c35-8d4b-42083804949a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generative AI Study Chatbot\n",
            "Type your question below.\n",
            "Type 'exit' to stop.\n",
            "\n",
            "\n",
            "Answer:\n",
            "RAG performance depends heavily on the relevance and accuracy of nretrieved documents.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n• Even a strong LLM will hallucinate if retrieval is weak.n•\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Answer:\n",
            "A Large Language Model (LLM) is a Foundation Model (FM) (a model pre-trained on massive data) that exclusively uses the Decoder-Only stack.\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Answer:\n",
            "Retrieval-nAugmented nGeneration n(RAG)nCombines retrieval and generation to use external data nfor more factual and context-aware responses.nWhen you want the model’s nresponses to include real-time, nrelevant information from nexternal sources.n• Even a strong LLM will hallucinate if retrieval is weak.n• Testing retrieval = testing the foundation of the entire RAG pipeline.nWhen you want the model’s nresponses to include real-time, nrelevant information from nexternal sources.n• Retrieval-nAugmented nGeneration n(RAG)nCombines retrieval and generation to use external data nfor more factual and context-aware responses.nWhen you want the model’s nresponses to include real-time, nrelevant information from nexternal sources.n• When you want the model’s nresponses to include real-time, nrelevant information from nexternal sources.n•\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Generative AI Study Chatbot\")\n",
        "print(\"Type your question below.\")\n",
        "print(\"Type 'exit' to stop.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_question = input(\"Your question: \")\n",
        "\n",
        "    if user_question.lower() == \"exit\":\n",
        "        print(\"Chatbot session ended.\")\n",
        "        break\n",
        "\n",
        "    answer = rag_chain.invoke(user_question)\n",
        "\n",
        "    print(\"\\nAnswer:\")\n",
        "    print(answer)\n",
        "    print(\"\\n\" + \"-\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB-xsHZcJ1XT"
      },
      "source": [
        "## Short Reflection\n",
        "\n",
        "The RAG-based study assistant performed effectively in retrieving relevant course content and generating grounded, context-aware answers, with prompt engineering playing a key role in improving explanation quality and handling indirect or rephrased questions. One limitation encountered was the selection of local open-source language models: *flan-t5-small* produced fast but often imprecise responses, while *mistralai/Mistral-7B-Instruct-v0.2* delivered stronger reasoning at the cost of very slow inference. After experimentation, *flan-t5-base* provided the best balance between accuracy and response time for local execution. The use of Retrieval-Augmented Generation significantly improved answer reliability by grounding responses in the course materials rather than relying on the model’s parametric knowledge, which reduced hallucinations and increased the relevance and consistency of the generated answers.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}